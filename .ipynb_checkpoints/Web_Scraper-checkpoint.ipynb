{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c75c1143-fdbd-4dbe-81a1-1744c073af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d49a5c18-1d48-4b16-96ab-9c976ab1b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arab = pd.read_csv('arab_links_script_2.csv')\n",
    "df_jordan = pd.read_csv('jordan_links_script_2.csv')\n",
    "df_etihad = pd.read_csv('etihad_links_script_2.csv')\n",
    "df_housing = pd.read_csv('housing_links_script_2.csv')\n",
    "df_central = pd.read_csv('central_links_script.csv')\n",
    "df_faq = pd.read_csv('faq.csv')\n",
    "df_full_housing = pd.read_csv('new_script.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8dd793a-8fda-43e6-9491-2c878168b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_housing = df_full_housing.head(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42edcd3-3174-44f6-979d-391ca12b5b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bba7c7d-5325-48fe-a061-5d07ddf6d582",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# dfs = [df_arab,df_jordan,df_etihad]\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# dfs = {'arab':df_arab,\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#        'jordan':df_jordan,\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#        'etihad':df_etihad,\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#        'housing':df_housing\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#       }\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m dfs = {\u001b[33m'\u001b[39m\u001b[33mfaq_test\u001b[39m\u001b[33m'\u001b[39m:\u001b[43mdf_f\u001b[49m}\n",
      "\u001b[31mNameError\u001b[39m: name 'df_f' is not defined"
     ]
    }
   ],
   "source": [
    "# dfs = [df_arab,df_jordan,df_etihad]\n",
    "# dfs = {'arab':df_arab,\n",
    "#        'jordan':df_jordan,\n",
    "#        'etihad':df_etihad,\n",
    "#        'housing':df_housing\n",
    "#       }\n",
    "dfs = {'full_housing':df_full_housing}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ae0ccb-09a4-43dd-847c-218e3a2bb332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36\",\n",
    "#     \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "#     \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "#     \"Connection\": \"keep-alive\"\n",
    "# }\n",
    "\n",
    "# session = requests.Session()\n",
    "# session.headers.update(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07cf43-d1bd-4e87-ad2c-1c4a2b0d9a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CSV = 'scraped_bank_data.csv'\n",
    "SCROLL = False\n",
    "WAIT_TIME = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330b1c8c-bb67-42de-8ab8-966596715472",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_argument('--headless=new')\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_argument('--window-size=1920,1080')\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()),options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e326de-0c7b-46b7-be00-aa632fa4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_page():\n",
    "    last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script('window.scrollTo(0,document.body.scrollHeight);')\n",
    "        time.sleep(WAIT_TIME/2)\n",
    "        new_height = driver.execute_script('return document.body.scrollHeight')\n",
    "\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height= new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc74a2-f445-4a9e-8cf5-2ed3a3c89c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bank, df in dfs.items():\n",
    "#     names, urls = df['Name'], df['URL']\n",
    "#     size = len(names)\n",
    "#     df['Text'] = None\n",
    "#     for i in range(size):\n",
    "#         url = urls[i]\n",
    "#         name = names[i]\n",
    "#         print(f'[ INFO: {bank} bank {i + 1}/{size}] Visiting {name} at {url}')\n",
    "\n",
    "#         try:\n",
    "#             driver.get(url)\n",
    "#             time.sleep(WAIT_TIME)\n",
    "#             if SCROLL:\n",
    "#                 scroll_page()\n",
    "#                 time.sleep(WAIT_TIME)\n",
    "#             html = driver.page_source\n",
    "\n",
    "#             soup = BeautifulSoup(html, 'html.parser')\n",
    "#             for tag in soup(['header','footer','nav','aside','script','style']):\n",
    "#                 tag.decompose()\n",
    "#             # text = soup.get_text(separator='\\n')\n",
    "#             for tag in soup.select('.social, .contact, #footer, #header'):\n",
    "#                 tag.decompose()\n",
    "\n",
    "#             text = soup.get_text(separator='\\n')\n",
    "#             text = text.replace('\\xa0',' ')\n",
    "#             text = '\\n'.join({line.strip() for line in text.splitlines() if line.strip()})\n",
    "#             if len(text) >= 1000:\n",
    "#                 df.at[i,'Text'] = text  \n",
    "\n",
    "#             print(f'[INFO: {bank} bank {i + 1}/{size}] Scraped {len(text)} from {url}')\n",
    "#         except Exception as e:\n",
    "#             print(f'[INFO: {bank} bank {i + 1}/{size}] Error scraping {url}: {e}')\n",
    "\n",
    "#     df = df[df['Text'] != None]\n",
    "#     output_file = f'{bank}_scraped_data_test.csv'\n",
    "#     df.to_csv(output_file)\n",
    "#     print(f'[INFO: {bank} bank {i + 1}/{size}] Saved Data in {output_file}')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16677efb-8f96-4ec1-9890-04fe08418a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_time = time.time()\n",
    "\n",
    "for bank, df in dfs.items():\n",
    "    bank_time = time.time()\n",
    "    names, urls = df['Name'], df['URL']\n",
    "    size = len(names)\n",
    "    df['Text'] = None\n",
    "    for i in range(size):\n",
    "        url = urls[i]\n",
    "        name = names[i]\n",
    "        print(f'[ INFO: {bank} bank {i + 1}/{size}] Visiting {name} at {url}')\n",
    "\n",
    "        try:\n",
    "            r = requests.get(url,timeout=10)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.text,'html.parser')\n",
    "\n",
    "            for tag in soup(['header','footer','nav','aside','script','style']):\n",
    "                tag.decompose()\n",
    "\n",
    "            text = soup.get_text(separator='\\n')\n",
    "            text = '\\n'.join([line.strip() for line in text.splitlines() if line.strip()])\n",
    "\n",
    "            if len(text) >= 1000:\n",
    "                df.at[i,'Text'] = text\n",
    "\n",
    "            print(f'[INFO: {bank} bank {i + 1}/{size}] Scraped {len(text)} from {url}')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'[INFO: {bank} bank {i + 1}/{size}] Error scraping {url}: {e}')\n",
    "\n",
    "    df = df[df['Text'] != None]\n",
    "    output_file = f'{bank}_scraped_data.csv'\n",
    "    df.to_csv(output_file)\n",
    "    bank_end = time.time() - bank_time\n",
    "    print(f'[INFO: {bank} time {bank_end:.2f}s] Saved Data in {output_file}')\n",
    "\n",
    "loop_end = time.time() - loop_time\n",
    "print(f'[INFO] Ended Scraping. Time taken {loop_end:.2f}s')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34658f1-81ef-431e-9bb5-dc88b7077d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439381ca-0f25-43dc-8548-ec45326ce035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
